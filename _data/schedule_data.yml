- date: tbd
  title: >
    Week 1: <strong>Course Introduction</strong>
  topics:
    - Course structure and outline
    - Intro to ML and NLP
  readings:
  extra readings:

- date: tbd
  title: >
    Week 2: <strong>ML and NLP Introduction</strong>
  topics:
    - More on ML and NLP
    - Quick intro to neural networks
    - Transformers
  readings:
    - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <br/>
    - <a href="Language Model Overview.pdf">Language Model Overview</a> <br/>
  extra readings:
    

- date: tbd
  title: >
    Week 3: <strong>Transformers</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>



- date: tbd
  title: >
    Week ?: <strong>Fine-tuning</strong>
  topics:
    - Methods
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Parameter-efficient Methods</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>RLHF</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Prompting Strategies</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Scaling Laws and Emergent Abilities</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Multi-modal Models</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2303.03378">PaLM-E&#58; An Embodied Multimodal Language Model</a> <br/>
    - <a href="https://arxiv.org/abs/2204.14198">Flamingo&#58; a Visual Language Model for Few-Shot Learning</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Applications to RL</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2302.06692">Guiding Pretraining in Reinforcement Learning with Large Language Models</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Ethics and Societal Impacts</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Retrieval-Augmented Generation</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Tool Use</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>LLMs as Agents</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>
