- date: tbd
  title: >
    Week 1: <strong>Course Introduction</strong>
  topics:
    - Course structure and outline
    - Intro to ML and NLP
  readings:
  extra readings:

- date: tbd
  title: >
    Week 2: <strong>ML and NLP Introduction</strong>
  topics:
    - More on ML and NLP
    - Quick intro to neural networks
    - Transformers
  readings:
    - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <br/>
    - <a href="Language Model Overview.pdf">Language Model Overview</a> <br/>
  extra readings:
    

- date: tbd
  title: >
    Week 3: <strong>Transformers</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>
    - <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> <br/>



- date: tbd
  title: >
    Week ?: <strong>Fine-tuning</strong>
  topics:
    - Methods
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a> <br/>
    - <a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a> <br/>
    - <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca&#58; A Strong, Replicable Instruction-Following Model</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2305.11206">LIMA&#58; Less Is More for Alignment</a> <br/>
    - <a href="https://arxiv.org/abs/2304.12244">WizardLM&#58; Empowering Large Language Models to Follow Complex Instructions</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Parameter-efficient Methods and other Tricks</strong>
  topics:
    - Optimizers, Gradient Accumulation, Batch Size, etc.
    - Parallelism Strategies
    - LoRA and other parameter-efficient methods
  readings:
    - <a href="https://arxiv.org/abs/2106.09685">LoRA&#58; Low-Rank Adaptation of Large Language Models</a> <br/>
    - <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one">Methods and tools for efficient training on a single GPU</a> <br/>
    - <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many">Efficient Training on Multiple GPUs</a> <br/>
  extra readings:
    - <a href="https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2">Fine-Tuning LLMs&#58; LoRA or Full-Parameter? An in-depth Analysis with Llama 2</a> <br/>
    - <a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning&#58; Optimizing Continuous Prompts for Generation</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>RLHF</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization&#58; Your Language Model is Secretly a Reward Model</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Prompting Strategies</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Scaling Laws and Emergent Abilities</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a> <br/>
    - <a href="https://arxiv.org/abs/2309.01809">Are Emergent Abilities in Large Language Models just In-Context Learning?</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Multi-modal Models</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2303.03378">PaLM-E&#58; An Embodied Multimodal Language Model</a> <br/>
    - <a href="https://arxiv.org/abs/2204.14198">Flamingo&#58; a Visual Language Model for Few-Shot Learning</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2209.06794">PaLI&#58; A Jointly-Scaled Multilingual Language-Image Model</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Applications to RL</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2302.06692">Guiding Pretraining in Reinforcement Learning with Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2307.15818">RT-2&#58; Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2302.02662">Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Ethics and Societal Impacts</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Retrieval-Augmented Generation</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Tool Use</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2302.04761">Toolformer&#58; Language Models Can Teach Themselves to Use Tools</a> <br/>
    - <a href="https://arxiv.org/abs/2308.00675">Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2205.12255">TALM&#58; Tool Augmented Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2307.16789">ToolLLM&#58; Facilitating Large Language Models to Master 16000+ Real-world APIs</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>LLMs as Agents</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2305.16291">Voyager&#58; An Open-Ended Embodied Agent with Large Language Models</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/2308.03688">AgentBench&#58; Evaluating LLMs as Agents</a> <br/>
