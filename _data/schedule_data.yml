- date: tbd
  title: >
    Week 1: <strong>Course Introduction</strong>
  topics:
    - Course structure and outline
    - Intro to ML and NLP
  readings:
  extra readings:

- date: tbd
  title: >
    Week 2: <strong>ML and NLP Introduction</strong>
  topics:
    - More on ML and NLP
    - Quick intro to neural networks
    - Transformers
  readings:
    - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <br/>
    - <a href="Language Model Overview.pdf">Language Model Overview</a> <br/>
  extra readings:
    

- date: tbd
  title: >
    Week 3: <strong>Transformers</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>



- date: tbd
  title: >
    Week ?: <strong>Fine-tuning</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>RLHF</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Scaling Laws and Emergent Abilities</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Multi-modal Models</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>Ethics and Societal Impacts</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>

- date: tbd
  title: >
    Week ?: <strong>LLMs as Agents</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>
