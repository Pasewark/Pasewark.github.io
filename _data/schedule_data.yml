- date: tbd
  title: >
    Week 1: <strong>Course Introduction</strong>
  topics:
    - Course structure and outline
    - Intro to ML and Language Modeling
  readings:
  extra-readings:

- date: tbd
  title: >
    Week 2: <strong>ML and LM Introduction</strong>
  topics:
    - More on ML and Language Modeling
    - Quick intro to neural networks
    - Transformers
  readings:
    - <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <br/>
    - <a href="Language-Model-Overview.pdf">Language Model Overview</a> <br/>
  extra-readings:
    

- date: tbd
  title: >
    Week 3: <strong>Transformers</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> <br/>
    - <a href="https://github.com/karpathy/minGPT">Karpathy Transformer-decoder implementation</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/1810.04805">BERT&#58; Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br/>
    - <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> <br/>



- date: tbd
  title: >
    Week 4: <strong>Fine-tuning</strong>
  topics:
    - Methods
    - Instruction Tuning
  readings:
    - <a href="https://arxiv.org/abs/2109.01652">Finetuned Language Models Are Zero-Shot Learners</a> <br/>
    - <a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a> <br/>
    - <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca&#58; A Strong, Replicable Instruction-Following Model</a> <br/>
    - <a href="https://arxiv.org/abs/2112.00114">Show Your Work&#58; Scratchpads for Intermediate Computation with Language Models</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2305.11206">LIMA&#58; Less Is More for Alignment</a> <br/>
    - <a href="https://arxiv.org/abs/2304.12244">WizardLM&#58; Empowering Large Language Models to Follow Complex Instructions</a> <br/>
    - <a href="https://arxiv.org/abs/2305.15717">The False Promise of Imitating Proprietary LLMs</a> <br/>

- date: tbd
  title: >
    Week 5: <strong>Parameter-efficient Methods and other Tricks</strong>
  topics:
    - Optimizers, Gradient Accumulation, Batch Size, etc.
    - Parallelism Strategies
    - LoRA and other parameter-efficient methods
  readings:
    - <a href="https://arxiv.org/abs/2106.09685">LoRA&#58; Low-Rank Adaptation of Large Language Models</a> <br/>
    - <a href="https://huggingface.co/docs/transformers/perf_train_gpu_one">Methods and tools for efficient training on a single GPU</a> <br/>
    - <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many">Efficient Training on Multiple GPUs</a> <br/>
  extra-readings:
    - <a href="https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2">Fine-Tuning LLMs&#58; LoRA or Full-Parameter? An in-depth Analysis with Llama 2</a> <br/>
    - <a href="https://arxiv.org/abs/2101.00190">Prefix-Tuning&#58; Optimizing Continuous Prompts for Generation</a> <br/>

- date: tbd
  title: >
    Week 6: <strong>RLHF and more Fine-tuning</strong>
  topics:
    - RLHF
    - Data Quality
    - Do we need RL?
  readings:
    - <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2308.08998">Reinforced Self-Training (ReST) for Language Modeling</a> <br/>
    - <a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization&#58; Your Language Model is Secretly a Reward Model</a> <br/>

- date: tbd
  title: >
    Week 7: <strong>Prompting Strategies</strong>
  topics:
    - Few-shot learning
    - Other prompting strategies
  readings:
    - <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2202.12837">Rethinking the Role of Demonstrations&#58; What Makes In-Context Learning Work?</a> <br/>
    - <a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts&#58; Deliberate Problem Solving with Large Language Models</a> <br/>

- date: tbd
  title: >
    Week 8: <strong>Scaling Laws and Emergent Abilities</strong>
  topics:
    - Scaling Laws
    - Emergent Abilities
  readings:
    - <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2206.07682">Emergent Abilities of Large Language Models</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2304.15004">Are Emergent Abilities of Large Language Models a Mirage?</a> <br/>
    - <a href="https://arxiv.org/abs/2309.01809">Are Emergent Abilities in Large Language Models just In-Context Learning?</a> <br/>

- date: tbd
  title: >
    Week 9: <strong>Multi-modal Models</strong>
  topics:
    - Vision-Language Models
    - Other Multi-modal Models
  readings:
    - <a href="https://arxiv.org/abs/2303.03378">PaLM-E&#58; An Embodied Multimodal Language Model</a> <br/>
    - <a href="https://arxiv.org/abs/2204.14198">Flamingo&#58; a Visual Language Model for Few-Shot Learning</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2209.06794">PaLI&#58; A Jointly-Scaled Multilingual Language-Image Model</a> <br/>

- date: tbd
  title: >
    Week 10: <strong>Applications to RL</strong>
  topics:
    - Applications to Exploration
    - Integration with Robotics
  readings:
    - <a href="https://arxiv.org/abs/2302.06692">Guiding Pretraining in Reinforcement Learning with Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2307.15818">RT-2&#58; Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2302.02662">Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning</a> <br/>

- date: tbd
  title: >
    Week 11: <strong>Ethics and Societal Impacts</strong>
  topics:
    - Current Issues&#58; bias, jobs, etc.
    - Potential Future Issues
  readings:
    - <a href="https://arxiv.org/abs/2112.04359">Ethical and social risks of harm from Language Models</a> <br/>
    - <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">On the Dangers of Stochastic Parrots&#58; Can Language Models Be Too Big?</a> <br/>
    - <a href="https://arxiv.org/abs/2308.14840">Identifying and Mitigating the Security Risks of Generative AI</a> <br/>
  extra-readings:
    - <a href="https://www.science.org/doi/10.1126/science.adj5957">How do we know how smart AI systems are?</a> <br/>
    - <a href="https://yoshuabengio.org/2023/05/22/how-rogue-ais-may-arise/">How Rogue AIs may Arise</a> <br/>

- date: tbd
  title: >
    Week 12: <strong>Retrieval-Augmented Generation</strong>
  topics:
    - Retrieval-Augmented Generation
  readings:
    - <a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> <br/>
    - <a href="https://arxiv.org/abs/2208.03299">Atlas&#58; Few-shot Learning with Retrieval Augmented Language Models</a> <br/>
  extra-readings:

- date: tbd
  title: >
    Week 13: <strong>Tool Use</strong>
  topics:
    - Methods to Teach Tool Use
  readings:
    - <a href="https://arxiv.org/abs/2302.04761">Toolformer&#58; Language Models Can Teach Themselves to Use Tools</a> <br/>
    - <a href="https://arxiv.org/abs/2308.00675">Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2205.12255">TALM&#58; Tool Augmented Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2307.16789">ToolLLM&#58; Facilitating Large Language Models to Master 16000+ Real-world APIs</a> <br/>

- date: tbd
  title: >
    Week 14: <strong>LLMs as Agents</strong>
  topics:
    - Transformer code
    - Training
  readings:
    - <a href="https://arxiv.org/abs/2305.16291">Voyager&#58; An Open-Ended Embodied Agent with Large Language Models</a> <br/>
    - <a href="https://arxiv.org/abs/2304.03442">Generative Agents&#58; Interactive Simulacra of Human Behavior</a> <br/>
  extra-readings:
    - <a href="https://arxiv.org/abs/2308.03688">AgentBench&#58; Evaluating LLMs as Agents</a> <br/>
